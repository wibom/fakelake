---
title: "Elasticsearch"
subtitle: "play 1"
date: 
output:
  html_document:
    self_contained: TRUE
    highlight: pygments
toc: TRUE
toc_float: TRUE
toc_depth: 3
code_folding: show
highlight: tango
number_sections: TRUE
theme: paper
keep_md: no
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=TRUE, message=FALSE, warning=FALSE}
here::i_am("es_play/search_play1.Rmd")
library(here)
library(glue)
library(elastic)
library(tidyverse)

# Initialize ----
x <- elastic::connect(
  host = "es01", 
  transport_schema = "https", 
  user = "elastic", 
  pwd = "qqpp11--",
  cainfo = "/usr/share/elasticsearch/config/certs/ca/ca.crt"
)
#x$ping()

# Get indices ----
indices_all <- cat_indices(x, parse = TRUE, verbose = TRUE)$index
```

# Query 1

List all datasets and how they overlap in terms of individuals:

## a) get the filepath of each dataset
```{r}
# get unique values from specific field
# - https://stackoverflow.com/a/57297526/7439717
# - https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html
body <- '{
  "aggs": {
    "files": {
      "terms": { "field": "dataset.filename" }
    }
  }
}'
res <- Search(x, index = indices_all, body = body, source = FALSE)
filenames <- map_chr(res$aggregations$files$buckets, ~ ..1$key)

print(filenames)
```

## b) get individuals included in each dataset
```{r}
# get individuals from a file
get_individuals_in_dataset <- function(filename) {

  body <- glue(
    '{{
      "query": {{
        "bool": {{
          "filter": {{
            "term": {{
              "dataset.filename": "{filename}"
            }}
          }}
        }}
      }}, 
      "aggs": {{
        "individuals": {{
          "terms": {{"field": "predict_id", "size": 500}}
        }}
      }}
    }}'
  )
  res <- Search(x, index = indices_all, body = body, source = FALSE)
  individuals <- map_chr(res$aggregations$individuals$buckets, ~ ..1$key)  
}
individuals_per_file <- map(filenames, get_individuals_in_dataset)
```

```{r}
print(individuals_per_file)
```

> Huzza! This was what I was aiming for :)




# Query 2
**Get all samples characterized by Metabolon metabolomics**

```{r}
body <- '{
  "query": {
    "bool": {
      "filter": {
        "term": {
          "dataset.analytics_platform": "Metabolon"
        }
      }
    }
  }, 
  "aggs": {
    "files": {
      "terms": { "field": "dataset.filename" }
    }
  }  
}'
res <- Search(x, index = "datasets", body = body, source = FALSE)
filenames <- map_chr(res$aggregations$files$buckets, ~ ..1$key)

# Get event_id: predict_id | event_date | event_setting
# Add tally (there may be >1 document per event_id)

```



# Query 3
Get individuals with repeat events (i.e. events at >1 date, not ) in lake.

NOTE:   
"Events" may be defined differently, e.g. "health check" and "questionnaire" 
from the same date may be considered a single event, or multiple events. 

**Here**, I'm only looking at events occurring at more than one date, i.e. I am
considering `event_type = health` and `event_type = questionnaire` as one single
event if they occur on exactly the same date.



# Further thoughts

* Add an index for individuals that tracks consents and 'nej talong'`
<br>
* Are there any benefits of assigning dockument ids? 
    + Each document is currently assigned an `uuid` by ES upon loading indices
<br>
* Currently all mappmings are done dynamically - is there a need to do this more
  carefully (manually)?
<br>
* Perhaps fun to see if I can get SQL-queries to work
    + Using RJDBC: https://dzone.com/articles/analyze-elasticsearch-data-in-r
    + Using Dremeo: https://www.dremio.com/tutorials/joins-in-elasticsearch/


# SessionInfo
```{r}
sessionInfo()
```





