# PREDICT LAKE

A sandbox datalake to play with in order to figure out:

1) Elasticsearch indexing
    1) What to index
    1) How to formate indices
    1) How to query indices
    1) Etc...
1) datalake structure, data-flows, etc..

The `fakelake` folder contains mock data that mimic some data expected to be held within the actual lake.

## Filestructure

```bash
.
├── dev
│   ├── data_templates       # mock templates used
│   ├── meta_templates       # yaml settings to index
│   │   ├── nshds
│   │   ├── x1
│   │   ├── x2
│   │   └── x3
│   └── utilities
├── elk                      # Docker-compose file to setup env.
├── es_play
├── fakelake                 # data, generated by `dev/spinnup_lake.R`
│   ├── 1_bronze             # fake raw data
│   │   ├── nshds
│   │   │   ├── contracts
│   │   │   └── doc
│   │   ├── x1
│   │   │   ├── data_1
│   │   │   │   ├── contracts
│   │   │   │   └── doc
│   │   │   └── data_2
│   │   │       ├── contracts
│   │   │       └── doc
│   │   └── x2
│   │       └── data_1
│   │           ├── contracts
│   │           └── doc
│   ├── 2_silver             # fake metadata to index
│   │   ├── nshds
│   │   ├── x1
│   │   │   ├── data_1
│   │   │   └── data_2
│   │   └── x2
│   │       └── data_1
│   └── 3_gold               # fake ES indices (.ndjson)
│       ├── nshds
│       ├── x1
│       │   ├── data_1
│       │   └── data_2
│       └── x2
│           └── data_1
└── ref                      # reference used for createing mock-data
```

As a first attempt I've simply structured the data as:

1) `1_bronze`: raw data
2) `2_silver`: meta data (generated/curated by PREDICT stewards)
3) `3_gold`: Elasticsearch indices (e.g. metadata formated to .ndjson)


## (Re-) Generating `fakelake`
The mock data was generated using:

1. Run: `dev/spinnup_lake.R`
    - some lake settings are read from `dev/lake_settings.yaml`, as well as the yaml files under `dev/meta_templates`
    - other settings are hardcoded to `dev/spinunup_lake.R` and `dev/utilities/utilities.R`
        - initially all settings were hardcoded, and not all have been migrated out to yaml files... 

> Perhaps the same code, with appropriate changes, may be useful to re-create a new `fakelake` with more useful settings. 

>Iterate > update > find useful structure of both data and indices.


# Docker image with: Elasticsearch / Kibana / Rstudio 

Set up working environment including Elasticsearch, Kibana and Rstudio.

This can for example be done by:

1. Modify  the `elk/docker-compose-predict.yaml` & run `compose up`  
    * modify the `r / volumes:` value to map local path

2. Connect to Elasticsearch from R
    * this requiers allowing the default 'rstudio'-user read permissions on the Elasticsearch certificates files

Changing file permissions on the Elasticsearch certificates within the R-studio image can for example be achieved from the R-studio terminal, by:

```bash
# Navigate to the certificates-folder:  
`cd /usr/share/elasticsearch/`
# `chmod` the `certs` folder:
`rstudio@5556462f98de:/usr/share/elasticsearch$ sudo chmod -R 755 config/`
```

Test that the connection is working from R:
```R
# Initialize ----
x <- elastic::connect(
  host = "es01", 
  transport_schema = "https", 
  user = "elastic", 
  pwd = "qqpp11--",
  cainfo = "/usr/share/elasticsearch/config/certs/ca/ca.crt"
)
x$ping()
```

# Import indices to Elasticsearch
The suggested indice files (.ndjson) are under `fakelake/3_gold/...` (they are created by the `dev/spinnup_lake.R` script)

They can be bulk loaded into Elasticsearch (using dynamic mappings) by running the script: `dev/import_indices_to_es.R`


## Kibana dataviews

As an initial setting, I am playing the Kibana dataviews (a.k.a "index-patterns" in ES < 8.x), based on the index naming scheme used (see: `dev/lake_settings.yaml`)

* `events`
* `datasets`
* `*--analytes`
* `*--vardict`



# Play around

Pretending to query the `fakelake`:

1) `es_play/search_play1.Rmd`  
    output: `es_play/search_play1.html`

<br><br>

# Current issues

None at the moment :)

# Resolved issues :)
## Quering for filepaths
Using the Kibana `Dev Tools` console:

This works:
```bash
GET /_search
{
  "size": 0, 
  "query": {
    "query_string": {
      "default_field": "dataset.filename",
      "query": "/fakelake/1_bronze/nshds/nshds_mock.tsv"
    }
  },
  "aggs": {
    "individuals": {
      "terms": {"field": "predict_id.keyword", "size": 500}
    }
  }
}
```

This works:
```bash
GET /_search
{
  "size": 0,
  "query": {
    "query_string": {
      "default_field": "dataset.filename",
      "query": "x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX"
    }
  }, 
  "aggs": {
    "individuals": {
      "terms": {"field": "predict_id.keyword", "size": 500}
    }
  }
}
```

This **does not** work:
```bash
GET /_search
{
  "size": 0,
  "query": {
    "query_string": {
      "default_field": "dataset.filename",
      "query": "1_bronze/x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX"
    }
  }, 
  "aggs": {
    "individuals": {
      "terms": {"field": "predict_id.keyword", "size": 500}
    }
  }
}

# Further tests:
# query: "/fakelake/1_bronze/x2/data_1"      -- works
# query: "\/fakelake\/1_bronze\/x2\/data_1*" -- works
# query: "/fakelake/1_bronze/x2/data_1*"     -- works
# query: "/fakelake/1_bronze/x2/data_1/*"    -- error

# query: "x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX"          -- works
# query: "1_bronze/x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- error

# ==============================================================================
# More structured tests:
# query: "/fakelake/1_bronze/x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- error (original query)
# query: "fakelake/1_bronze/x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- ok

# query: "/1_bronze/x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- ok
# query: "1_bronze/x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- error

# query: "/x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- error
# query: "x2/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- ok

# query: "/data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- ok
# query: "data_1/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- error

# query: "/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- error
# query: "mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- ok

# Conclusion: 
# - works with even number of '/'
# - errors with odd number of '/'

# query: "\/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- error
# query: "\\/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- ok  ==> need double escape of '/'

# original query with double escape:
# query: "\\/fakelake\\/1_bronze\\/x2\\/data_1\\/mock_UMEA-01-19ML+CDT_EDTA_PLASMA_02FEB1792.XLSX" -- OK!
```

### Solution

* Forward slash ('/')  must be double escaped when used in query
* Also, `query_string` might not be the best option, probably better to use `bool` instead (see: [Filter and aggregate])


## Filter and aggregate

### Issue
```bash
# --------------------------
# I was hoping this query would only return the individuals included in 
# the file "/fakelake/1_bronze/x1/data_2/mock_Olink_NPX_1791-02-02.csv"
#
# Instead all included individuals in the lake are returned... 
# How do I reformulate the query to return only individuals in file?
GET /_search
{
  "size": 0,
  "query": {
    "query_string": {
      "default_field": "dataset.filename",
      "query": "\\/fakelake\\/1_bronze\\/x1\\/data_2\\/mock_Olink_NPX_1791-02-02.csv"
    }
  },
  "aggs": {
    "individuals": {
      "terms": {"field": "predict_id.keyword", "size": 500}
    }
  }
}
```

### Solution
Apparently `query_string` performes some sort of text-search that I don't 
understand.

Using boolean search does what I want:

```bash
# --------------------------
GET /_search
{
  "query": {
    "bool": {
      "filter": {
        "term": {
          "dataset.filename.keyword": "/fakelake/1_bronze/x1/data_2/mock_Olink_NPX_1791-02-02.csv"
        }
      }
    }
  }, 
  "aggs": {
    "individuals": {
      "terms": {"field": "predict_id.keyword", "size": 500}
    }    
  }
}
```

NOTE: this way it's not necessary to escape forward slash characters.